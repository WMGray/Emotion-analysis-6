{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Conda\\envs\\d2l\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import numpy as np\n",
    "import yaml\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "maxlen = 20"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_dictionaries(model=None, combined=None):\n",
    "    \"\"\"创建词语字典，并返回每个词语的索引，词向量，以及每个句子所对应的词语索引\"\"\"\n",
    "    if (combined is not None) and (model is not None):\n",
    "        if os.path.exists(Config.combined_path) and os.path.exists(Config.w2vec_path) and os.path.exists(\n",
    "                Config.w2indx_path):  # 已写入\n",
    "            print(\"加载词典\")\n",
    "            w2indx_json = codecs.open(Config.w2indx_path, 'r', encoding=Config.encoding)\n",
    "            w2vec_json = codecs.open(Config.w2vec_path, 'r', encoding=Config.encoding)\n",
    "            combined_json = codecs.open(Config.combined_path, 'r', encoding=Config.encoding)\n",
    "\n",
    "            w2indx = json.load(w2indx_json)\n",
    "            W2VEC = json.load(w2vec_json)  # 加载的数组为list，需转换为numppy数组\n",
    "            combined = np.loadtxt(combined_json, dtype=int)\n",
    "\n",
    "            # 转换\n",
    "            w2vec = dict()\n",
    "            for key, value in W2VEC.items():\n",
    "                w2vec[key] = np.asarray(value)\n",
    "\n",
    "            w2indx_json.close()\n",
    "            w2vec_json.close()\n",
    "            combined_json.close()\n",
    "\n",
    "            return w2indx, w2vec, combined\n",
    "        else:\n",
    "            gensim_dict = Dictionary()  # 创建一个空的词典,构建 word<->id 映射\n",
    "            gensim_dict.doc2bow(list(model.wv.index_to_key),\n",
    "                                allow_update=True)  # 构建词袋，每个单词对应一个id，词袋中的单词不重复\n",
    "            w2indx = {v: k + 1 for k, v in gensim_dict.items()}  # 所有频数超过10的词语的索引字典\n",
    "            w2vec = {word: model.wv[word] for word in w2indx.keys()}  # 所有频数超过10的词语的词向量字典\n",
    "            W2VEC = {word: model.wv[word].tolist() for word in w2indx.keys()}  # 将numpy数组转换为list存储\n",
    "\n",
    "            def parse_dataset(combined):\n",
    "                \"\"\"将combined中的数据转换为索引表示\"\"\"\n",
    "                data = []\n",
    "                for sentence in combined:\n",
    "                    new_txt = []\n",
    "                    for word in sentence:\n",
    "                        try:\n",
    "                            new_txt.append(w2indx[word])\n",
    "                        except:\n",
    "                            new_txt.append(0)\n",
    "                    new_txt=torch.Tensor(new_txt[:maxlen]).int()\n",
    "                    data.append(new_txt)\n",
    "                return data\n",
    "\n",
    "            combined = parse_dataset(combined)  # 将combined中的数据转换为索引表示\n",
    "            combined = nn.utils.rnn.pad_sequence(combined, batch_first=True, padding_value=0)\n",
    "            # 每个句子所含词语对应的索引，所以句子中含有频数小于10的词语，索引为0\n",
    "\n",
    "            w2indx_json = codecs.open(Config.w2indx_path, 'w', encoding=Config.encoding)\n",
    "            w2vec_json = codecs.open(Config.w2vec_path, 'w', encoding=Config.encoding)\n",
    "            combined_json = codecs.open(Config.combined_path, 'w', encoding=Config.encoding)\n",
    "\n",
    "            json.dump(w2indx, w2indx_json)\n",
    "            json.dump(W2VEC, w2vec_json)\n",
    "            np.savetxt(combined_json, combined)  # numpy.ndarrayi\n",
    "\n",
    "            w2indx_json.close()\n",
    "            w2vec_json.close()\n",
    "            combined_json.close()\n",
    "\n",
    "            return w2indx, w2vec, combined\n",
    "    else:\n",
    "        print('No data provided...')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def input_transform(string):\n",
    "    words = jieba.lcut(string)\n",
    "    words = np.array(words).reshape(1, -1)\n",
    "    model = Word2Vec.load('word2vec/Word2vec_model.pkl')\n",
    "    _, _, combined = create_dictionaries(model, words)\n",
    "    return combined"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
