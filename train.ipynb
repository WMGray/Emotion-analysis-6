{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "import json\n",
    "import multiprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.cuda.amp import autocast\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "\n",
    "from config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "seed = 2022\n",
    "val_ratio = 0.2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_everything(seed)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "vocab_dim = 512  # 词向量维度\n",
    "maxlen = 20  # 序列最大长度\n",
    "n_iterations = 50  # 迭代次数\n",
    "n_exposures = 5  # 词频截断值\n",
    "window_size = 5  # 窗口大小\n",
    "batch_size = 32  # 批次大小\n",
    "n_epoch = 25  # 迭代次数\n",
    "input_length = 20  # 输入序列长度\n",
    "cpu_count = multiprocessing.cpu_count()  #\n",
    "window_sizes = [3, 3, 5, 7]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def load_file():\n",
    "    \"\"\"加载数据\"\"\"\n",
    "    # 加载数据集\n",
    "    null = pd.read_csv(Config.Emotion_List[0], header=None, index_col=None)  # null\n",
    "    like = pd.read_csv(Config.Emotion_List[1], header=None, index_col=None)  # like\n",
    "    sad = pd.read_csv(Config.Emotion_List[2], header=None, index_col=None)  # sad\n",
    "    disgust = pd.read_csv(Config.Emotion_List[3], header=None, index_col=None)  # disgust\n",
    "    anger = pd.read_csv(Config.Emotion_List[4], header=None, index_col=None)  # anger\n",
    "    happy = pd.read_csv(Config.Emotion_List[5], header=None, index_col=None) # happy\n",
    "\n",
    "\n",
    "    null = [row[0].split() for row in null.values]\n",
    "    like = [row[0].split() for row in like.values]\n",
    "    sad = [row[0].split() for row in sad.values]\n",
    "    disgust = [row[0].split() for row in disgust.values]\n",
    "    anger = [row[0].split() for row in anger.values]\n",
    "    happy = [row[0].split() for row in happy.values]\n",
    "\n",
    "    # 拼接\n",
    "    x = null + like + sad + disgust + anger + happy\n",
    "    print(type(x))\n",
    "    # null-0 like-1 sad-2 disgust-3 anger-4 happy-5\n",
    "    y = np.concatenate((np.zeros(len(null), dtype=int), np.ones(len(like), dtype=int),\n",
    "                        np.ones(len(sad), dtype=int) * 2, np.ones(len(disgust), dtype=int) * 3,\n",
    "                        np.ones(len(anger), dtype=int) * 4, np.ones(len(happy), dtype=int) * 5))\n",
    "\n",
    "    return x, y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def create_dictionaries(model=None, combined=None):\n",
    "    \"\"\"创建词语字典，并返回每个词语的索引，词向量，以及每个句子所对应的词语索引\"\"\"\n",
    "    if (combined is not None) and (model is not None):\n",
    "        if os.path.exists(Config.combined_path) and os.path.exists(Config.w2vec_path) and os.path.exists(\n",
    "                Config.w2indx_path):  # 已写入\n",
    "            print(\"加载词典\")\n",
    "            w2indx_json = codecs.open(Config.w2indx_path, 'r', encoding=Config.encoding)\n",
    "            w2vec_json = codecs.open(Config.w2vec_path, 'r', encoding=Config.encoding)\n",
    "            combined_json = codecs.open(Config.combined_path, 'r', encoding=Config.encoding)\n",
    "\n",
    "            w2indx = json.load(w2indx_json)\n",
    "            W2VEC = json.load(w2vec_json)  # 加载的数组为list，需转换为numppy数组\n",
    "            combined = np.loadtxt(combined_json, dtype=int)\n",
    "\n",
    "            # 转换\n",
    "            w2vec = dict()\n",
    "            for key, value in W2VEC.items():\n",
    "                w2vec[key] = np.asarray(value)\n",
    "\n",
    "            w2indx_json.close()\n",
    "            w2vec_json.close()\n",
    "            combined_json.close()\n",
    "\n",
    "            return w2indx, w2vec, combined\n",
    "        else:\n",
    "            gensim_dict = Dictionary()  # 创建一个空的词典,构建 word<->id 映射\n",
    "            gensim_dict.doc2bow(list(model.wv.index_to_key),\n",
    "                                allow_update=True)  # 构建词袋，每个单词对应一个id，词袋中的单词不重复\n",
    "            w2indx = {v: k + 1 for k, v in gensim_dict.items()}  # 所有频数超过10的词语的索引字典\n",
    "            w2vec = {word: model.wv[word] for word in w2indx.keys()}  # 所有频数超过10的词语的词向量字典\n",
    "            W2VEC = {word: model.wv[word].tolist() for word in w2indx.keys()}  # 将numpy数组转换为list存储\n",
    "\n",
    "            def parse_dataset(combined):\n",
    "                \"\"\"将combined中的数据转换为索引表示\"\"\"\n",
    "                data = []\n",
    "                for sentence in combined:\n",
    "                    new_txt = []\n",
    "                    for word in sentence:\n",
    "                        try:\n",
    "                            new_txt.append(w2indx[word])\n",
    "                        except:\n",
    "                            new_txt.append(0)\n",
    "                    new_txt=torch.Tensor(new_txt[:maxlen]).int()\n",
    "                    data.append(new_txt)\n",
    "                return data\n",
    "\n",
    "            combined = parse_dataset(combined)  # 将combined中的数据转换为索引表示\n",
    "            combined = nn.utils.rnn.pad_sequence(combined, batch_first=True, padding_value=0)\n",
    "            # 每个句子所含词语对应的索引，所以句子中含有频数小于10的词语，索引为0\n",
    "\n",
    "            w2indx_json = codecs.open(Config.w2indx_path, 'w', encoding=Config.encoding)\n",
    "            w2vec_json = codecs.open(Config.w2vec_path, 'w', encoding=Config.encoding)\n",
    "            combined_json = codecs.open(Config.combined_path, 'w', encoding=Config.encoding)\n",
    "\n",
    "            json.dump(w2indx, w2indx_json)\n",
    "            json.dump(W2VEC, w2vec_json)\n",
    "            np.savetxt(combined_json, combined)  # numpy.ndarrayi\n",
    "\n",
    "            w2indx_json.close()\n",
    "            w2vec_json.close()\n",
    "            combined_json.close()\n",
    "\n",
    "            return w2indx, w2vec, combined\n",
    "    else:\n",
    "        print('No data provided...')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def word2vec_train(combined):\n",
    "    \"\"\"创建词语字典，并返回每个词语的索引，词向量，以及每个句子所对应的词语索引\"\"\"\n",
    "    # size：是指特征向量的维度  min_count:对字典做截断. 词频少于min_count次数的单词会被丢弃掉\n",
    "    # window：表示当前词与预测词在一个句子中的最大距离是多少  workers：参数控制训练的并行数。\n",
    "    # iter： 迭代次数，默认为5\n",
    "    print(combined[:10])\n",
    "    if not os.path.exists(Config.word2vec_path):\n",
    "        print(111)\n",
    "        model = Word2Vec(\n",
    "            sentences=combined,\n",
    "            vector_size=vocab_dim,\n",
    "            min_count=n_exposures,\n",
    "            window=window_size,\n",
    "            workers=cpu_count,\n",
    "            epochs=n_iterations)\n",
    "        model.save(Config.word2vec_path)  # 保存模型\n",
    "    else:\n",
    "        print(\"load word2vec model\")\n",
    "        model = Word2Vec.load(Config.word2vec_path)\n",
    "\n",
    "    index_dict, word_vectors, combined = create_dictionaries(model=model, combined=combined)\n",
    "    return index_dict, word_vectors, combined"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def get_data(index_dict, word_vectors, combined, y):\n",
    "    n_symbols = len(index_dict) + 1  # 所有单词的索引数，频数小于10的词语索引为0，所以加1\n",
    "    embedding_weights = np.zeros((n_symbols, vocab_dim))  # 索引为0的词语，词向量全为0\n",
    "    for word, index in index_dict.items():  # 从索引为1的词语开始，对每个词语对应其词向量\n",
    "        embedding_weights[index, :] = word_vectors[word]\n",
    "    x_train, x_test, y_train, y_test = train_test_split(combined, y, test_size=0.2)\n",
    "\n",
    "    print(f\"train {x_train.shape}, test {x_test.shape}\")\n",
    "\n",
    "    return n_symbols, embedding_weights, x_train, y_train, x_test, y_test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "class MYDataset(Dataset):\n",
    "    def __init__(self, x, y, index_dict, word_vectors):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.n_symbols = len(index_dict) + 1  # 所有单词的索引数，频数小于10的词语索引为0，所以加1\n",
    "        self.embedding_weights = np.zeros((self.n_symbols, vocab_dim))  # 索引为0的词语，词向量全为0\n",
    "\n",
    "        for word, index in index_dict.items():  # 从索引为1的词语开始，对每个词语对应其词向量\n",
    "            self.embedding_weights[index, :] = word_vectors[word]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        labels = torch.zeros(6)\n",
    "        labels[self.y[idx]] = 1\n",
    "\n",
    "        # print(self.x[idx])\n",
    "        return self.embedding_weights[self.x[idx]], labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.convs = nn.ModuleList([\n",
    "                nn.Sequential(nn.Conv1d(in_channels=vocab_dim,\n",
    "                                        out_channels=256,\n",
    "                                        kernel_size=h),\n",
    "                              nn.Dropout(0.2),\n",
    "                              nn.BatchNorm1d(num_features=256),\n",
    "                              nn.ReLU(),\n",
    "                              nn.MaxPool1d(kernel_size=maxlen-h+1))\n",
    "                     for h in window_sizes\n",
    "                    ])\n",
    "        self.fc = nn.Linear(in_features=256 * len(window_sizes),\n",
    "                            out_features=6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # inputs = [batch, maxlen, vocab_dim]\n",
    "\n",
    "        # SpatialDropout1D\n",
    "        x = x.permute(0, 2, 1)   #  [batch, vocab_dim, maxlen]\n",
    "        x = F.dropout2d(x, 0.3, training=self.training)\n",
    "        x = x.permute(0, 2, 1)   # back to  [batch, maxlen, vocab_dim]\n",
    "\n",
    "        # batch_size x text_len x embedding_size  -> batch_size x embedding_size x text_len\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        out = torch.cat([conv(x).squeeze(-1) for conv in self.convs], dim=1)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data...\n",
      "<class 'list'>\n",
      "2168771 2168771\n",
      "Training a Word2vec model...\n",
      "[['又', '翠湖', \"'\", '？', '呀', '秋', '哥', \"'\", '你', '讲', '过', '好多', '次', '了'], ['无非', '是', '那', '两', '个', '人', '吧', '…'], ['义工', '是', '做', '什么', '的', '嫩'], ['围', '湖', '跑', '吗', '，', '大概', '是', '几', '圈'], ['没有', '二', '次', '基本', '没有', '后期', '调整'], ['组团', '儿', '去', '当', '尼姑', '吧'], ['新', '电影', '1', '月', '1', '号', '上', '，', '专辑', '年底', '制作', '。'], ['跟', '安藤', '有的', '一', '拼', '啦', '！'], ['听说', '点', '，', '我家', '那边', '就是', '主', '战场'], ['星期', '一', '到', '北京', '。']]\n",
      "load word2vec model\n",
      "加载词典\n",
      "get data and labels\n"
     ]
    }
   ],
   "source": [
    "# 训练模型，并保存\n",
    "print('Loading Data...')\n",
    "combined, y = load_file()\n",
    "print(len(combined), len(y))\n",
    "\n",
    "print('Training a Word2vec model...')\n",
    "index_dict, word_vectors, combined = word2vec_train(combined)\n",
    "\n",
    "print(\"get data and labels\")\n",
    "dataset = MYDataset(combined, y, index_dict, word_vectors)\n",
    "\n",
    "val_size = int(len(dataset) * val_ratio)\n",
    "train_size = len(dataset) - val_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size]) # (1735017, 433754)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "TextCNN(\n  (convs): ModuleList(\n    (0): Sequential(\n      (0): Conv1d(512, 256, kernel_size=(3,), stride=(1,))\n      (1): Dropout(p=0.2, inplace=False)\n      (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (3): ReLU()\n      (4): MaxPool1d(kernel_size=18, stride=18, padding=0, dilation=1, ceil_mode=False)\n    )\n    (1): Sequential(\n      (0): Conv1d(512, 256, kernel_size=(3,), stride=(1,))\n      (1): Dropout(p=0.2, inplace=False)\n      (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (3): ReLU()\n      (4): MaxPool1d(kernel_size=18, stride=18, padding=0, dilation=1, ceil_mode=False)\n    )\n    (2): Sequential(\n      (0): Conv1d(512, 256, kernel_size=(5,), stride=(1,))\n      (1): Dropout(p=0.2, inplace=False)\n      (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (3): ReLU()\n      (4): MaxPool1d(kernel_size=16, stride=16, padding=0, dilation=1, ceil_mode=False)\n    )\n    (3): Sequential(\n      (0): Conv1d(512, 256, kernel_size=(7,), stride=(1,))\n      (1): Dropout(p=0.2, inplace=False)\n      (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (3): ReLU()\n      (4): MaxPool1d(kernel_size=14, stride=14, padding=0, dilation=1, ceil_mode=False)\n    )\n  )\n  (fc): Linear(in_features=1024, out_features=6, bias=True)\n)"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TextCNN()\n",
    "model.cuda()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss_fn = nn.CrossEntropyLoss().to('cuda')\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3,\n",
    "                                      gamma=0.4)  # reduce the learning after 20 epochs by a factor of 10"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "def train(model, epoch):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    epoch_acc = 0.0\n",
    "    with tqdm(train_loader, unit=\"batch\") as tepoch: # 🌟 1. 定义进度条\n",
    "        for inputs, labels in tepoch:   # 🌟 2. 设置迭代器\n",
    "            tepoch.set_description(f\"Epoch {epoch} train: \") # 🌟 3. 设置开头\n",
    "\n",
    "            inputs, labels = inputs.type(torch.cuda.FloatTensor).cuda(), labels.cuda()\n",
    "\n",
    "            with autocast():\n",
    "                outputs = model(inputs)\n",
    "                # print(outputs, labels)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                correct = (outputs.argmax(axis=1) == labels.argmax(axis=1)).sum().item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += correct\n",
    "\n",
    "        train_acc = epoch_acc / len(train_loader.dataset)\n",
    "        # print(epoch_acc, len(train_loader.dataset))\n",
    "        train_loss = epoch_loss / len(train_loader.dataset)\n",
    "\n",
    "        tepoch.write(\"Train Epoch: {} Train Loss: {:.6f} Train Acc: {:.6f}\".format(epoch, train_loss, train_acc))\n",
    "        tepoch.close()\n",
    "\n",
    "        return train_acc, train_loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "def test(model, epoch):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval() # set the model to evaluation mode\n",
    "    with tqdm(val_loader, unit=\"batch\") as vepoch: # 🌟 1. 定义进度条\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in vepoch:   # 🌟 2. 设置迭代器\n",
    "                vepoch.set_description(f\"Epoch {epoch} val: \") # 🌟 3. 设置开头\n",
    "                inputs, labels = inputs.type(torch.cuda.FloatTensor).cuda(), labels.cuda()\n",
    "\n",
    "                outputs = model(inputs).squeeze(1)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                # 计算准确率\n",
    "                correct = (outputs.argmax(axis=1) == labels.argmax(axis=1)).sum().item()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "                epoch_acc += correct\n",
    "\n",
    "        val_acc = epoch_acc / len(val_loader.dataset)\n",
    "        val_loss = epoch_loss / len(val_loader.dataset)\n",
    "\n",
    "        vepoch.write(\"Test Epoch: {} Test Loss {:.6f} Test Accuracy: {:.6f}\\n\".format(epoch, val_loss, val_acc))\n",
    "        vepoch.close()\n",
    "\n",
    "        return val_acc, val_loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "train_accs, train_losses, val_accs, val_losses = [], [], [], []"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 train: :  94%|█████████▍| 51108/54219 [06:07<00:21, 141.68batch/s]"
     ]
    }
   ],
   "source": [
    "model_path =\"model/model.pth\"\n",
    "for epoch in range(1, n_epoch + 1):\n",
    "    train_acc, train_loss = train(model, epoch)\n",
    "    val_acc, val_loss = test(model, epoch)\n",
    "\n",
    "    train_accs.append(train_acc), train_losses.append(train_loss)\n",
    "    val_accs.append(val_acc), val_losses.append(val_loss)\n",
    "\n",
    "    scheduler.step()\n",
    "    if val_acc >= best_acc:\n",
    "        best_acc = val_acc\n",
    "        torch.save({'epoch': epoch, 'state_dict': model.state_dict()}, model_path)\n",
    "        print('Epoch: {} saving model with Acc {:.3f}'.format(epoch, best_acc))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
