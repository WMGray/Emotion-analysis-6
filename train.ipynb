{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "import json\n",
    "import multiprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.cuda.amp import autocast\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "\n",
    "from config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "seed = 2022\n",
    "val_ratio = 0.2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_everything(seed)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "vocab_dim = 512  # è¯å‘é‡ç»´åº¦\n",
    "maxlen = 20  # åºåˆ—æœ€å¤§é•¿åº¦\n",
    "n_iterations = 50  # è¿­ä»£æ¬¡æ•°\n",
    "n_exposures = 5  # è¯é¢‘æˆªæ–­å€¼\n",
    "window_size = 5  # çª—å£å¤§å°\n",
    "batch_size = 32  # æ‰¹æ¬¡å¤§å°\n",
    "n_epoch = 25  # è¿­ä»£æ¬¡æ•°\n",
    "input_length = 20  # è¾“å…¥åºåˆ—é•¿åº¦\n",
    "cpu_count = multiprocessing.cpu_count()  #\n",
    "window_sizes = [3, 3, 5, 7]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def load_file():\n",
    "    \"\"\"åŠ è½½æ•°æ®\"\"\"\n",
    "    # åŠ è½½æ•°æ®é›†\n",
    "    null = pd.read_csv(Config.Emotion_List[0], header=None, index_col=None)  # null\n",
    "    like = pd.read_csv(Config.Emotion_List[1], header=None, index_col=None)  # like\n",
    "    sad = pd.read_csv(Config.Emotion_List[2], header=None, index_col=None)  # sad\n",
    "    disgust = pd.read_csv(Config.Emotion_List[3], header=None, index_col=None)  # disgust\n",
    "    anger = pd.read_csv(Config.Emotion_List[4], header=None, index_col=None)  # anger\n",
    "    happy = pd.read_csv(Config.Emotion_List[5], header=None, index_col=None) # happy\n",
    "\n",
    "\n",
    "    null = [row[0].split() for row in null.values]\n",
    "    like = [row[0].split() for row in like.values]\n",
    "    sad = [row[0].split() for row in sad.values]\n",
    "    disgust = [row[0].split() for row in disgust.values]\n",
    "    anger = [row[0].split() for row in anger.values]\n",
    "    happy = [row[0].split() for row in happy.values]\n",
    "\n",
    "    # æ‹¼æ¥\n",
    "    x = null + like + sad + disgust + anger + happy\n",
    "    print(type(x))\n",
    "    # null-0 like-1 sad-2 disgust-3 anger-4 happy-5\n",
    "    y = np.concatenate((np.zeros(len(null), dtype=int), np.ones(len(like), dtype=int),\n",
    "                        np.ones(len(sad), dtype=int) * 2, np.ones(len(disgust), dtype=int) * 3,\n",
    "                        np.ones(len(anger), dtype=int) * 4, np.ones(len(happy), dtype=int) * 5))\n",
    "\n",
    "    return x, y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def create_dictionaries(model=None, combined=None):\n",
    "    \"\"\"åˆ›å»ºè¯è¯­å­—å…¸ï¼Œå¹¶è¿”å›æ¯ä¸ªè¯è¯­çš„ç´¢å¼•ï¼Œè¯å‘é‡ï¼Œä»¥åŠæ¯ä¸ªå¥å­æ‰€å¯¹åº”çš„è¯è¯­ç´¢å¼•\"\"\"\n",
    "    if (combined is not None) and (model is not None):\n",
    "        if os.path.exists(Config.combined_path) and os.path.exists(Config.w2vec_path) and os.path.exists(\n",
    "                Config.w2indx_path):  # å·²å†™å…¥\n",
    "            print(\"åŠ è½½è¯å…¸\")\n",
    "            w2indx_json = codecs.open(Config.w2indx_path, 'r', encoding=Config.encoding)\n",
    "            w2vec_json = codecs.open(Config.w2vec_path, 'r', encoding=Config.encoding)\n",
    "            combined_json = codecs.open(Config.combined_path, 'r', encoding=Config.encoding)\n",
    "\n",
    "            w2indx = json.load(w2indx_json)\n",
    "            W2VEC = json.load(w2vec_json)  # åŠ è½½çš„æ•°ç»„ä¸ºlistï¼Œéœ€è½¬æ¢ä¸ºnumppyæ•°ç»„\n",
    "            combined = np.loadtxt(combined_json, dtype=int)\n",
    "\n",
    "            # è½¬æ¢\n",
    "            w2vec = dict()\n",
    "            for key, value in W2VEC.items():\n",
    "                w2vec[key] = np.asarray(value)\n",
    "\n",
    "            w2indx_json.close()\n",
    "            w2vec_json.close()\n",
    "            combined_json.close()\n",
    "\n",
    "            return w2indx, w2vec, combined\n",
    "        else:\n",
    "            gensim_dict = Dictionary()  # åˆ›å»ºä¸€ä¸ªç©ºçš„è¯å…¸,æ„å»º word<->id æ˜ å°„\n",
    "            gensim_dict.doc2bow(list(model.wv.index_to_key),\n",
    "                                allow_update=True)  # æ„å»ºè¯è¢‹ï¼Œæ¯ä¸ªå•è¯å¯¹åº”ä¸€ä¸ªidï¼Œè¯è¢‹ä¸­çš„å•è¯ä¸é‡å¤\n",
    "            w2indx = {v: k + 1 for k, v in gensim_dict.items()}  # æ‰€æœ‰é¢‘æ•°è¶…è¿‡10çš„è¯è¯­çš„ç´¢å¼•å­—å…¸\n",
    "            w2vec = {word: model.wv[word] for word in w2indx.keys()}  # æ‰€æœ‰é¢‘æ•°è¶…è¿‡10çš„è¯è¯­çš„è¯å‘é‡å­—å…¸\n",
    "            W2VEC = {word: model.wv[word].tolist() for word in w2indx.keys()}  # å°†numpyæ•°ç»„è½¬æ¢ä¸ºlistå­˜å‚¨\n",
    "\n",
    "            def parse_dataset(combined):\n",
    "                \"\"\"å°†combinedä¸­çš„æ•°æ®è½¬æ¢ä¸ºç´¢å¼•è¡¨ç¤º\"\"\"\n",
    "                data = []\n",
    "                for sentence in combined:\n",
    "                    new_txt = []\n",
    "                    for word in sentence:\n",
    "                        try:\n",
    "                            new_txt.append(w2indx[word])\n",
    "                        except:\n",
    "                            new_txt.append(0)\n",
    "                    new_txt=torch.Tensor(new_txt[:maxlen]).int()\n",
    "                    data.append(new_txt)\n",
    "                return data\n",
    "\n",
    "            combined = parse_dataset(combined)  # å°†combinedä¸­çš„æ•°æ®è½¬æ¢ä¸ºç´¢å¼•è¡¨ç¤º\n",
    "            combined = nn.utils.rnn.pad_sequence(combined, batch_first=True, padding_value=0)\n",
    "            # æ¯ä¸ªå¥å­æ‰€å«è¯è¯­å¯¹åº”çš„ç´¢å¼•ï¼Œæ‰€ä»¥å¥å­ä¸­å«æœ‰é¢‘æ•°å°äº10çš„è¯è¯­ï¼Œç´¢å¼•ä¸º0\n",
    "\n",
    "            w2indx_json = codecs.open(Config.w2indx_path, 'w', encoding=Config.encoding)\n",
    "            w2vec_json = codecs.open(Config.w2vec_path, 'w', encoding=Config.encoding)\n",
    "            combined_json = codecs.open(Config.combined_path, 'w', encoding=Config.encoding)\n",
    "\n",
    "            json.dump(w2indx, w2indx_json)\n",
    "            json.dump(W2VEC, w2vec_json)\n",
    "            np.savetxt(combined_json, combined)  # numpy.ndarrayi\n",
    "\n",
    "            w2indx_json.close()\n",
    "            w2vec_json.close()\n",
    "            combined_json.close()\n",
    "\n",
    "            return w2indx, w2vec, combined\n",
    "    else:\n",
    "        print('No data provided...')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def word2vec_train(combined):\n",
    "    \"\"\"åˆ›å»ºè¯è¯­å­—å…¸ï¼Œå¹¶è¿”å›æ¯ä¸ªè¯è¯­çš„ç´¢å¼•ï¼Œè¯å‘é‡ï¼Œä»¥åŠæ¯ä¸ªå¥å­æ‰€å¯¹åº”çš„è¯è¯­ç´¢å¼•\"\"\"\n",
    "    # sizeï¼šæ˜¯æŒ‡ç‰¹å¾å‘é‡çš„ç»´åº¦  min_count:å¯¹å­—å…¸åšæˆªæ–­. è¯é¢‘å°‘äºmin_countæ¬¡æ•°çš„å•è¯ä¼šè¢«ä¸¢å¼ƒæ‰\n",
    "    # windowï¼šè¡¨ç¤ºå½“å‰è¯ä¸é¢„æµ‹è¯åœ¨ä¸€ä¸ªå¥å­ä¸­çš„æœ€å¤§è·ç¦»æ˜¯å¤šå°‘  workersï¼šå‚æ•°æ§åˆ¶è®­ç»ƒçš„å¹¶è¡Œæ•°ã€‚\n",
    "    # iterï¼š è¿­ä»£æ¬¡æ•°ï¼Œé»˜è®¤ä¸º5\n",
    "    print(combined[:10])\n",
    "    if not os.path.exists(Config.word2vec_path):\n",
    "        print(111)\n",
    "        model = Word2Vec(\n",
    "            sentences=combined,\n",
    "            vector_size=vocab_dim,\n",
    "            min_count=n_exposures,\n",
    "            window=window_size,\n",
    "            workers=cpu_count,\n",
    "            epochs=n_iterations)\n",
    "        model.save(Config.word2vec_path)  # ä¿å­˜æ¨¡å‹\n",
    "    else:\n",
    "        print(\"load word2vec model\")\n",
    "        model = Word2Vec.load(Config.word2vec_path)\n",
    "\n",
    "    index_dict, word_vectors, combined = create_dictionaries(model=model, combined=combined)\n",
    "    return index_dict, word_vectors, combined"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def get_data(index_dict, word_vectors, combined, y):\n",
    "    n_symbols = len(index_dict) + 1  # æ‰€æœ‰å•è¯çš„ç´¢å¼•æ•°ï¼Œé¢‘æ•°å°äº10çš„è¯è¯­ç´¢å¼•ä¸º0ï¼Œæ‰€ä»¥åŠ 1\n",
    "    embedding_weights = np.zeros((n_symbols, vocab_dim))  # ç´¢å¼•ä¸º0çš„è¯è¯­ï¼Œè¯å‘é‡å…¨ä¸º0\n",
    "    for word, index in index_dict.items():  # ä»ç´¢å¼•ä¸º1çš„è¯è¯­å¼€å§‹ï¼Œå¯¹æ¯ä¸ªè¯è¯­å¯¹åº”å…¶è¯å‘é‡\n",
    "        embedding_weights[index, :] = word_vectors[word]\n",
    "    x_train, x_test, y_train, y_test = train_test_split(combined, y, test_size=0.2)\n",
    "\n",
    "    print(f\"train {x_train.shape}, test {x_test.shape}\")\n",
    "\n",
    "    return n_symbols, embedding_weights, x_train, y_train, x_test, y_test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "class MYDataset(Dataset):\n",
    "    def __init__(self, x, y, index_dict, word_vectors):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.n_symbols = len(index_dict) + 1  # æ‰€æœ‰å•è¯çš„ç´¢å¼•æ•°ï¼Œé¢‘æ•°å°äº10çš„è¯è¯­ç´¢å¼•ä¸º0ï¼Œæ‰€ä»¥åŠ 1\n",
    "        self.embedding_weights = np.zeros((self.n_symbols, vocab_dim))  # ç´¢å¼•ä¸º0çš„è¯è¯­ï¼Œè¯å‘é‡å…¨ä¸º0\n",
    "\n",
    "        for word, index in index_dict.items():  # ä»ç´¢å¼•ä¸º1çš„è¯è¯­å¼€å§‹ï¼Œå¯¹æ¯ä¸ªè¯è¯­å¯¹åº”å…¶è¯å‘é‡\n",
    "            self.embedding_weights[index, :] = word_vectors[word]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        labels = torch.zeros(6)\n",
    "        labels[self.y[idx]] = 1\n",
    "\n",
    "        # print(self.x[idx])\n",
    "        return self.embedding_weights[self.x[idx]], labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.convs = nn.ModuleList([\n",
    "                nn.Sequential(nn.Conv1d(in_channels=vocab_dim,\n",
    "                                        out_channels=256,\n",
    "                                        kernel_size=h),\n",
    "                              nn.Dropout(0.2),\n",
    "                              nn.BatchNorm1d(num_features=256),\n",
    "                              nn.ReLU(),\n",
    "                              nn.MaxPool1d(kernel_size=maxlen-h+1))\n",
    "                     for h in window_sizes\n",
    "                    ])\n",
    "        self.fc = nn.Linear(in_features=256 * len(window_sizes),\n",
    "                            out_features=6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # inputs = [batch, maxlen, vocab_dim]\n",
    "\n",
    "        # SpatialDropout1D\n",
    "        x = x.permute(0, 2, 1)   #  [batch, vocab_dim, maxlen]\n",
    "        x = F.dropout2d(x, 0.3, training=self.training)\n",
    "        x = x.permute(0, 2, 1)   # back to  [batch, maxlen, vocab_dim]\n",
    "\n",
    "        # batch_size x text_len x embedding_size  -> batch_size x embedding_size x text_len\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        out = torch.cat([conv(x).squeeze(-1) for conv in self.convs], dim=1)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data...\n",
      "<class 'list'>\n",
      "2168771 2168771\n",
      "Training a Word2vec model...\n",
      "[['åˆ', 'ç¿ æ¹–', \"'\", 'ï¼Ÿ', 'å‘€', 'ç§‹', 'å“¥', \"'\", 'ä½ ', 'è®²', 'è¿‡', 'å¥½å¤š', 'æ¬¡', 'äº†'], ['æ— é', 'æ˜¯', 'é‚£', 'ä¸¤', 'ä¸ª', 'äºº', 'å§', 'â€¦'], ['ä¹‰å·¥', 'æ˜¯', 'åš', 'ä»€ä¹ˆ', 'çš„', 'å«©'], ['å›´', 'æ¹–', 'è·‘', 'å—', 'ï¼Œ', 'å¤§æ¦‚', 'æ˜¯', 'å‡ ', 'åœˆ'], ['æ²¡æœ‰', 'äºŒ', 'æ¬¡', 'åŸºæœ¬', 'æ²¡æœ‰', 'åæœŸ', 'è°ƒæ•´'], ['ç»„å›¢', 'å„¿', 'å»', 'å½“', 'å°¼å§‘', 'å§'], ['æ–°', 'ç”µå½±', '1', 'æœˆ', '1', 'å·', 'ä¸Š', 'ï¼Œ', 'ä¸“è¾‘', 'å¹´åº•', 'åˆ¶ä½œ', 'ã€‚'], ['è·Ÿ', 'å®‰è—¤', 'æœ‰çš„', 'ä¸€', 'æ‹¼', 'å•¦', 'ï¼'], ['å¬è¯´', 'ç‚¹', 'ï¼Œ', 'æˆ‘å®¶', 'é‚£è¾¹', 'å°±æ˜¯', 'ä¸»', 'æˆ˜åœº'], ['æ˜ŸæœŸ', 'ä¸€', 'åˆ°', 'åŒ—äº¬', 'ã€‚']]\n",
      "load word2vec model\n",
      "åŠ è½½è¯å…¸\n",
      "get data and labels\n"
     ]
    }
   ],
   "source": [
    "# è®­ç»ƒæ¨¡å‹ï¼Œå¹¶ä¿å­˜\n",
    "print('Loading Data...')\n",
    "combined, y = load_file()\n",
    "print(len(combined), len(y))\n",
    "\n",
    "print('Training a Word2vec model...')\n",
    "index_dict, word_vectors, combined = word2vec_train(combined)\n",
    "\n",
    "print(\"get data and labels\")\n",
    "dataset = MYDataset(combined, y, index_dict, word_vectors)\n",
    "\n",
    "val_size = int(len(dataset) * val_ratio)\n",
    "train_size = len(dataset) - val_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size]) # (1735017, 433754)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "TextCNN(\n  (convs): ModuleList(\n    (0): Sequential(\n      (0): Conv1d(512, 256, kernel_size=(3,), stride=(1,))\n      (1): Dropout(p=0.2, inplace=False)\n      (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (3): ReLU()\n      (4): MaxPool1d(kernel_size=18, stride=18, padding=0, dilation=1, ceil_mode=False)\n    )\n    (1): Sequential(\n      (0): Conv1d(512, 256, kernel_size=(3,), stride=(1,))\n      (1): Dropout(p=0.2, inplace=False)\n      (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (3): ReLU()\n      (4): MaxPool1d(kernel_size=18, stride=18, padding=0, dilation=1, ceil_mode=False)\n    )\n    (2): Sequential(\n      (0): Conv1d(512, 256, kernel_size=(5,), stride=(1,))\n      (1): Dropout(p=0.2, inplace=False)\n      (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (3): ReLU()\n      (4): MaxPool1d(kernel_size=16, stride=16, padding=0, dilation=1, ceil_mode=False)\n    )\n    (3): Sequential(\n      (0): Conv1d(512, 256, kernel_size=(7,), stride=(1,))\n      (1): Dropout(p=0.2, inplace=False)\n      (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (3): ReLU()\n      (4): MaxPool1d(kernel_size=14, stride=14, padding=0, dilation=1, ceil_mode=False)\n    )\n  )\n  (fc): Linear(in_features=1024, out_features=6, bias=True)\n)"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TextCNN()\n",
    "model.cuda()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss_fn = nn.CrossEntropyLoss().to('cuda')\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3,\n",
    "                                      gamma=0.4)  # reduce the learning after 20 epochs by a factor of 10"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "def train(model, epoch):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    epoch_acc = 0.0\n",
    "    with tqdm(train_loader, unit=\"batch\") as tepoch: # ğŸŒŸ 1. å®šä¹‰è¿›åº¦æ¡\n",
    "        for inputs, labels in tepoch:   # ğŸŒŸ 2. è®¾ç½®è¿­ä»£å™¨\n",
    "            tepoch.set_description(f\"Epoch {epoch} train: \") # ğŸŒŸ 3. è®¾ç½®å¼€å¤´\n",
    "\n",
    "            inputs, labels = inputs.type(torch.cuda.FloatTensor).cuda(), labels.cuda()\n",
    "\n",
    "            with autocast():\n",
    "                outputs = model(inputs)\n",
    "                # print(outputs, labels)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                correct = (outputs.argmax(axis=1) == labels.argmax(axis=1)).sum().item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += correct\n",
    "\n",
    "        train_acc = epoch_acc / len(train_loader.dataset)\n",
    "        # print(epoch_acc, len(train_loader.dataset))\n",
    "        train_loss = epoch_loss / len(train_loader.dataset)\n",
    "\n",
    "        tepoch.write(\"Train Epoch: {} Train Loss: {:.6f} Train Acc: {:.6f}\".format(epoch, train_loss, train_acc))\n",
    "        tepoch.close()\n",
    "\n",
    "        return train_acc, train_loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "def test(model, epoch):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval() # set the model to evaluation mode\n",
    "    with tqdm(val_loader, unit=\"batch\") as vepoch: # ğŸŒŸ 1. å®šä¹‰è¿›åº¦æ¡\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in vepoch:   # ğŸŒŸ 2. è®¾ç½®è¿­ä»£å™¨\n",
    "                vepoch.set_description(f\"Epoch {epoch} val: \") # ğŸŒŸ 3. è®¾ç½®å¼€å¤´\n",
    "                inputs, labels = inputs.type(torch.cuda.FloatTensor).cuda(), labels.cuda()\n",
    "\n",
    "                outputs = model(inputs).squeeze(1)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                # è®¡ç®—å‡†ç¡®ç‡\n",
    "                correct = (outputs.argmax(axis=1) == labels.argmax(axis=1)).sum().item()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "                epoch_acc += correct\n",
    "\n",
    "        val_acc = epoch_acc / len(val_loader.dataset)\n",
    "        val_loss = epoch_loss / len(val_loader.dataset)\n",
    "\n",
    "        vepoch.write(\"Test Epoch: {} Test Loss {:.6f} Test Accuracy: {:.6f}\\n\".format(epoch, val_loss, val_acc))\n",
    "        vepoch.close()\n",
    "\n",
    "        return val_acc, val_loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "train_accs, train_losses, val_accs, val_losses = [], [], [], []"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 train: :  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 51108/54219 [06:07<00:21, 141.68batch/s]"
     ]
    }
   ],
   "source": [
    "model_path =\"model/model.pth\"\n",
    "for epoch in range(1, n_epoch + 1):\n",
    "    train_acc, train_loss = train(model, epoch)\n",
    "    val_acc, val_loss = test(model, epoch)\n",
    "\n",
    "    train_accs.append(train_acc), train_losses.append(train_loss)\n",
    "    val_accs.append(val_acc), val_losses.append(val_loss)\n",
    "\n",
    "    scheduler.step()\n",
    "    if val_acc >= best_acc:\n",
    "        best_acc = val_acc\n",
    "        torch.save({'epoch': epoch, 'state_dict': model.state_dict()}, model_path)\n",
    "        print('Epoch: {} saving model with Acc {:.3f}'.format(epoch, best_acc))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
